{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import laspy\n",
    "\n",
    "with laspy.open(r\"E:\\RTE_entrainement\\IA_Train_Valid_Test\\Dev_YSO_py\\pred_25.laz\") as fh:\n",
    "    print('Points from Header:', fh.header.point_count)\n",
    "    las = fh.read()\n",
    "    print(las)\n",
    "    print(las.xyz.shape)\n",
    "    print('Points from data:', len(las.points))\n",
    "    unique_classes,class_counts= np.unique(las.classification, return_counts=True)\n",
    "    class_counts = dict(zip(unique_classes, class_counts))\n",
    "    print(unique_classes)\n",
    "    print(len(unique_classes))\n",
    "    ground_pts = las.classification\n",
    "   \n",
    "    \n",
    "    bins_g, counts_g = np.unique(las.return_number[ground_pts], return_counts=True)\n",
    "    print('Ground Point Return Number distribution:')\n",
    "    for r,c in zip(bins_g,counts_g):\n",
    "        print('  {}:{}'.format(r,c))\n",
    "        \n",
    "    for clas in unique_classes:\n",
    "        print(f'{clas},\":\", {las.xyz}')\n",
    "        \n",
    "    print(ground_pts)\n",
    "    print(las.points)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main script for metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import laspy\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score,jaccard_score\n",
    "import matplotlib.pyplot as plt \n",
    "from tqdm import tqdm\n",
    "\n",
    "# Here, we're going to set, useful fonctions or potentialy useful \n",
    "\n",
    "def fIoU (tp,fp,fn) :\n",
    "    return tp / (tp + fp + fn)\n",
    "#intersection over union\n",
    "\n",
    "\n",
    "# Foncton to calculate the precision of our model prediction\n",
    "def Metricsecall (tp,fn) :\n",
    "    \n",
    "    #nombre : nombre de points classifiés dans une certaine classe par le nuage de référence\n",
    "    if (tp+fn) == 0 :\n",
    "        return 0\n",
    "    else :\n",
    "        return tp/(tp+fn)\n",
    "    \n",
    "def faccuracy(tp,fp,fn):\n",
    "    return tp/(tp+fp+fn)\n",
    "    \n",
    "#nombre de points dans la même classification (les true points en cas de nuage de référence) / nombre de point de la classe \n",
    "#spécifique dans le nuage 1\n",
    "#qualitativement c'est la proportion de points correctements classifiés par rapport aux points dans la classif\n",
    "# recall == 1 signifie que l'algo n'omet pas de points\n",
    "\n",
    "\n",
    "# Foncton to calculate the precision of our model prediction\n",
    "\n",
    "def fprecision (tp,fp) :\n",
    "    if (tp+fp) == 0 :\n",
    "        return 0\n",
    "    else :\n",
    "        return tp/(tp+fp)\n",
    "# precision == 1 signifie que que l'algo ne prend pas de points en trop\n",
    "\n",
    "# here, is for a specific comparison between precision and the recall \n",
    "def ff1score (precision,rappel) :\n",
    "    if (precision+rappel) < 10**-5 :\n",
    "        return 0\n",
    "    return 2*(precision*rappel)/(precision+rappel)\n",
    "\n",
    "\n",
    "# =======This fonction retutrns general inforrmation about the file=====================#\n",
    "#========possible to edit the script in oder to define your own output willing==========#\n",
    "\n",
    "def infos_class(file_path):\n",
    "    with laspy.open(file_path) as fh:\n",
    "        print('Points Metricsom Header:', fh.header.point_count)\n",
    "        las = fh.read()\n",
    "        print(las.xyz.shape)\n",
    "        print('Points Metricsom data:', len(las.points))\n",
    "        unique_classes,class_counts= np.unique(las.classification, return_counts=True)\n",
    "        class_counts = dict(zip(unique_classes, class_counts))\n",
    "        print(class_counts)\n",
    "        #print(las.classification)\n",
    "        #  Display number of return\n",
    "        \n",
    "        \"\"\"ground_pts= las.classification\n",
    "        bins_g, counts_g = np.unique(las.return_number[ground_pts], return_counts=True)\n",
    "        print('Ground Point Return Number distribution:')\n",
    "        for r,c in zip(bins_g,counts_g):\n",
    "            print('    {}:{}'.format(r,c))\"\"\"\n",
    "\n",
    "\n",
    "# =============This part is only for reading las files====================================#\n",
    "# ==============It returns something like this: with the number of points=================#\n",
    "#=<LasData(1.4, point fmt: <PointFormat(6, 8 bytes of extra dims)>, 1258912 points, 1 vlrs)>=#\n",
    "\n",
    "def read_file(file_path):\n",
    "    with laspy.open(file_path) as fh:\n",
    "        data=fh.read()\n",
    "    return data\n",
    "#\n",
    "# ===================reading data and  extracting specific information,===================# \n",
    "# ====================like the occurences of classes in the file==========================#\n",
    "\n",
    "def read_file_class(file_path):\n",
    "    with laspy.open(file_path) as fh:\n",
    "        data=fh.read()\n",
    "    return data.classification\n",
    "#\n",
    "#====== Textract data label (different existing classes inside las files)=================#\n",
    "\n",
    "def data_label(las):\n",
    "    unique_classes,class_counts= np.unique(las.classification, return_counts=True)\n",
    "    class_counts = dict(zip(unique_classes, class_counts))    \n",
    "    return unique_classes\n",
    "\n",
    "# this fonction is for Extracting Data 3D cordonnates Metricsom laZ files\n",
    "def label_xyz(las):\n",
    "    label_xyz = las.xyz\n",
    "    return label_xyz\n",
    "\n",
    "\n",
    "# ==============this fonction allow to count number of points in a class===============#\n",
    "\n",
    "def count_points_per_class(file_path):\n",
    "    class_counts = {}\n",
    "    with laspy.open(file_path) as fh:\n",
    "        las = fh.read()\n",
    "        unique_classes, class_counts = np.unique(las.classification, return_counts=True)\n",
    "        class_counts = dict(zip(unique_classes, class_counts))\n",
    "    return class_counts\n",
    "\n",
    "\n",
    "#\n",
    "# ==========Extract each label of class in its occurence in the global point cloud======#\n",
    "\n",
    "def num_points(classe):\n",
    "    num_point_clas=[]\n",
    "    for el in tqdm(range(len(classe))):\n",
    "        num_point_clas.append((classe[el]))\n",
    "    return num_point_clas\n",
    "\n",
    "# ====================Compute matrix of confusion======================================#\n",
    "\n",
    "\n",
    "def compute_confusion_matrix(ground_truth, predictions):\n",
    "    \"\"\"\n",
    "    Calcule la matrice de confusion pour les prédictions et les classes réelles.\n",
    "    Args:\n",
    "        predictions (numpy.ndarray): Tableau des prédictions du modèle.\n",
    "        ground_truth (numpy.ndarray): Tableau des classes réelles.\n",
    "    Returns:\n",
    "        pandas.DataFrame: Matrice de confusion avec les index et\n",
    "        colonnes représentant les classes.\n",
    "    \"\"\"\n",
    "    # Identifier les classes uniques dans ground_truth et predictions\n",
    "    unique_classes = np.unique(np.concatenate((predictions, ground_truth)))\n",
    "    \n",
    "    # Créer une matrice de confusion basée uniquement sur les classes uniques\n",
    "    confusion_matrix = np.zeros((len(unique_classes), len(unique_classes)))\n",
    "    print(\"len ground_truth\",len(ground_truth))\n",
    "    print(\"len pred\",len(predictions))\n",
    "    # Remplir la matrice de confusion uniquement pour les classes existantes\n",
    "    for i in tqdm(range(len(predictions))):\n",
    "        pred_class = predictions[i]\n",
    "        true_class = ground_truth[i]\n",
    "        \n",
    "        # Vérifier si la prédiction a une classe correspondante dans les données réelles\n",
    "        if true_class in unique_classes:\n",
    "            pred_index = np.where(unique_classes == pred_class)[0][0]\n",
    "            true_index = np.where(unique_classes == true_class)[0][0]\n",
    "            confusion_matrix[pred_index, true_index] += 1\n",
    "\n",
    "    # Créer des titres de classe basés sur les classes uniques\n",
    "    class_titles = [f\"Classe {c}\" for c in unique_classes]\n",
    "\n",
    "    # Créer le DataFrame de la matrice de confusion avec les titres de classe\n",
    "    conf_matrix_df = pd.DataFrame(confusion_matrix, index=class_titles, columns=class_titles)\n",
    "    \n",
    "    return conf_matrix_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Metrics:\n",
    "    \n",
    "    #\n",
    "    # ====================Initializing my class Metrics================================================#\n",
    "    \n",
    "    def __init__(self, true_labels, pred_labels) :\n",
    "        self.true_labels=true_labels\n",
    "        self.pred_labels=pred_labels\n",
    "    \n",
    "    #\n",
    "    # ====================Display metrics simply as a dictionnary======================================#    \n",
    "    def compute_metrics(self):\n",
    "        \"\"\" return metrics by class\"\"\"\n",
    "        IoU=jaccard_score(self.true_labels, self.pred_labels, average=None)\n",
    "        precision = precision_score(self.true_labels, self.pred_labels, average=None,zero_division='warn')\n",
    "        recall = recall_score(self.true_labels, self.pred_labels, average=None,zero_division='warn')\n",
    "        f1 = f1_score(self.true_labels, self.pred_labels, average=None)\n",
    "        accuracy = accuracy_score(self.true_labels, self.pred_labels)\n",
    "        \n",
    "        metrics_dict = {\n",
    "        'IoU': IoU,\n",
    "        'Recall': recall,\n",
    "        'Precision': precision,\n",
    "        'F1_score': f1,\n",
    "        'Accuracy': accuracy\n",
    "    }\n",
    "        return metrics_dict\n",
    "    # \n",
    "    \n",
    "    # ====================Display metrics bay class as a frame  without weighting =====================#\n",
    "    \n",
    "    def metrics_frame(self,results):\n",
    "        unique_classes = np.unique(np.concatenate((self.true_labels, self.pred_labels)))\n",
    "        # Créez des titres de classe basés sur les classes uniques\n",
    "        class_titles = [f\"Classe {c}\" for c in unique_classes]\n",
    "        nam_metrics = ['IoU', 'Recall', 'Precision', 'F1_score', 'Accuracy']\n",
    "        Metrics = np.zeros((5, len(results['IoU'])))\n",
    "        \n",
    "        for i, metric_name in enumerate(nam_metrics):\n",
    "            Metrics[i] = results[metric_name]    \n",
    "        #  \n",
    "        # Créez le DataMetricsame de la matrice de confusion avec les titres de classe\n",
    "        Metrics_df = pd.DataFrame(Metrics, index=nam_metrics, columns=class_titles)\n",
    "        Metrics_df.index.name = 'Métriques'\n",
    "        Metrics_df.columns.name = 'Valeurs des métriques'\n",
    "        return Metrics_df\n",
    "        \n",
    "     # ====================Display metrics as a frame by weghting each class===========================#   \n",
    "    def weighted_metrics(self):\n",
    "        \n",
    "        \"\"\"Calculate metrics for each label, and find their average weighted \n",
    "        by support (the number of true instances for each label)\"\"\"\n",
    "        \n",
    "        IoU_w=jaccard_score(self.true_labels, self.pred_labels, average=\"weighted\")\n",
    "        precision_w = precision_score(self.true_labels, self.pred_labels, average='weighted',zero_division='warn')\n",
    "        recall_w = recall_score(self.true_labels, self.pred_labels, average='weighted',zero_division='warn')\n",
    "        f1_w = f1_score(self.true_labels, self.pred_labels, average='weighted')\n",
    "        accuracy = accuracy_score(self.true_labels, self.pred_labels)\n",
    "        \n",
    "        nam_metrics = ['IoU', 'Recall','precision','Accuracy', 'F1_score']\n",
    "        weigh_metrics=[IoU_w,recall_w,precision_w,f1_w,accuracy]\n",
    "        weigh_metrics_df=pd.DataFrame({'Valeurs': weigh_metrics}, index=nam_metrics)\n",
    "        weigh_metrics_df.index.name = 'Métriques'\n",
    "        \n",
    "        return weigh_metrics_df\n",
    "        \n",
    "        \n",
    "     # ====================Display metrics as a graphics curves======================================#   \n",
    "        \n",
    "    def graphmetrics(self,metrics_data):        \n",
    "        data=np.arange(1, len(metrics_data['IoU'])+1)\n",
    "        plt.figure(figsize=(15,7))\n",
    "        # Tracé des métriques\n",
    "        for metric, values in metrics_data.items():\n",
    "            if metric != 'Accuracy':             \n",
    "                plt.plot(data, values, label=metric)\n",
    "        \n",
    "        # Ajout de légendes et de titres\n",
    "        plt.xlabel('Classes')\n",
    "        plt.ylabel('Valeur')\n",
    "        plt.title('Métriques par classe')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "            # Affichage du graphique\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "            \n",
    "    # ====================Display metrics as subplots graphics on grids===============================#\n",
    "    \n",
    "    def n_graphmetrics(self, metrics_data):        \n",
    "        data = np.arange(1, len(metrics_data['IoU']) + 1)\n",
    "        metrics_to_plot = [metric for metric in metrics_data if metric != 'Accuracy']\n",
    "        num_metrics = len(metrics_to_plot)\n",
    "        \n",
    "        fig, axes = plt.subplots(num_metrics, 1, figsize=(15, 7*num_metrics))\n",
    "        \n",
    "        for i, (metric, values) in enumerate(metrics_data.items()):\n",
    "            if metric != 'Accuracy':\n",
    "                ax = axes[i] if num_metrics > 1 else axes  # Utiliser le même axe s'il n'y a qu'une seule métrique\n",
    "                ax.plot(data, values, label=metric)\n",
    "                ax.set_xlabel('Classes')\n",
    "                ax.set_ylabel('Valeur')\n",
    "                ax.set_title(f'Métrique: {metric}')\n",
    "                ax.legend()\n",
    "                ax.grid(True)  # Ajout de la grille\n",
    "                \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Filter metrics according to the models config\n",
    "        \n",
    "    def filt_metrics(data_frame, model):\n",
    "       model2 = {\"0\":1, \"1\":2, \"2\":5, \"3\":6, \"4\":196, \"5\":211}\n",
    "       class_names = {\"Défaut\":1,\"Sol\": 2, \"Végégtation\":5,\"Bâtiments\":6, \"Structures\": 196, \"Câbles\": 211 }\n",
    "             \n",
    "\n",
    "                    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': 1, '1': 2, '2': 5, '3': 6, '4': 196, '5': 211}\n"
     ]
    }
   ],
   "source": [
    "model_dict = {\"1\":\"default\",\"2\":\"ground\",\"5\": \"vegetation\", \"6\":\"building\",\"7\": \"wall\", \"196\": \"structure\",\"211\":\"cable\"}\n",
    "\n",
    "conv_class = {\"0\":\"default\", \"1\": \"ground\", \"2\": \"vegetation\", \"3\": \"building\", \"4\": \"structure\", \"5\": \"cable\"}\n",
    "\n",
    "conv_dict2 = {\"1\": 0, \"2\": 1, \"3\": 0, \"4\": 2, \"5\": 2, \"6\": 3, \"7\": 6,  \"12\": 0, \"13\": 0, \"15\": 0, \"50\": 0,\n",
    "     \"51\": 0, \"52\": 0, \"53\": 0,\"54\": 0, \"55\": 0,  \"64\": 0,\"72\": 0,\"101\": 0,\"102\": 4,\"103\": 4, \"104\": 4,\n",
    "      \"105\": 4,\"106\": 0,\"107\": 0,\"108\": 0,\"109\": 0, \"110\": 0,\"111\": 0, \"120\": 0, \"121\": 0,\"150\": 0,\n",
    "      \"151\": 0,\"152\": 5,\"153\": 5,\"154\": 5, \"155\": 5,\"160\": 0,\"161\": 0,\"185\": 0,\"196\": 4,\"197\": 4,\n",
    "      \"200\": 5,\"201\": 5,\"202\": 5,\"211\": 5,\"212\": 5, \"213\": 5,\"221\": 5,\"222\": 5, \"223\": 5\n",
    "     }\n",
    "\n",
    "model = {k:int(l) for k, v in conv_class.items() for l,m in model_dict.items() if v == m}\n",
    "#model2 = {\"0\":1, \"1\":2, \"2\":5, \"3\":6, \"4\":196, \"5\":211}\n",
    "direct_conv2={k:j for k, v in conv_dict2.items() for  i, j in model.items()  if int(i) == v }\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = read_file_class(r\"E:/S_Données_pour apprentissage/data_output inference/Donnees  labelises en sortie d'inference/rte2 correctement labelise/99385bdd1b4245d6813754f478bd5f2c/preds/000006.laz\")\n",
    "print(np.unique(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infos_class(r\"E:/S_Données_pour apprentissage/data_output inference/Donnees  labelises en sortie d'inference/rte1 correctement labelise/db3ea63f541a4d9fbe68bd96f98a86ae/preds/000006.laz\")\n",
    "read_file_class(r\"E:/S_Données_pour apprentissage/data_output inference/Donnees  labelises en sortie d'inference/rte1 correctement labelise/db3ea63f541a4d9fbe68bd96f98a86ae/preds/000006.laz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test part for understanding the issue of mappind the input data with json file config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  6  50  60  64  67  68  79 102 103 104 105 106 107 108 109 110 111 150\n",
      " 151 152 153 154 155 160 161 196 197 200 201 211 212 213 221 222 223]\n",
      "<class 'numpy.ndarray'>\n",
      "{'1': 0, '2': 1, '3': 0, '4': 2, '5': 2, '6': 3, '7': 8, '12': 0, '13': 0, '15': 0, '50': 4, '51': 4, '52': 4, '53': 6, '54': 4, '55': 4, '64': 4, '72': 7, '101': 5, '102': 5, '103': 5, '104': 5, '105': 5, '106': 5, '107': 5, '108': 5, '109': 5, '110': 5, '111': 5, '120': 5, '121': 5, '150': 6, '151': 6, '152': 6, '154': 6, '155': 6, '160': 6, '161': 6, '185': 6, '196': 5, '197': 5, '200': 6, '201': 6, '202': 6, '211': 6, '212': 6, '213': 6, '221': 6, '222': 6, '223': 6}\n",
      "{'1': 1, '2': 2, '3': 1, '4': 5, '5': 5, '6': 6, '12': 1, '13': 1, '15': 1, '50': 59, '51': 59, '52': 59, '53': 211, '54': 59, '55': 59, '64': 59, '72': 72, '101': 196, '102': 196, '103': 196, '104': 196, '105': 196, '106': 196, '107': 196, '108': 196, '109': 196, '110': 196, '111': 196, '120': 196, '121': 196, '150': 211, '151': 211, '152': 211, '154': 211, '155': 211, '160': 211, '161': 211, '185': 211, '196': 196, '197': 196, '200': 211, '201': 211, '202': 211, '211': 211, '212': 211, '213': 211, '221': 211, '222': 211, '223': 211}\n",
      "\n",
      "unique values\n",
      "<class 'numpy.ndarray'>\n",
      "[  6  59 196 211]\n",
      "<class 'numpy.ndarray'>\n",
      "[  3   4   5   6  60  67  68  79 153]\n",
      "[ 0  3  4  5 60 67 68 79]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = read_file_class(r\"E:/S_Données_pour apprentissage/Données pour prédiction sans bruit/Controle lignes elec-1_YAM.laz\")\n",
    "print(np.unique(data))\n",
    "print(type(data))\n",
    "\n",
    "conv_dict1 = {\"1\": 0,\"2\": 1,\"3\": 0,\"4\": 2,\"5\": 2, \"6\": 3,\"7\": 8,\"12\": 0,\"13\": 0,\"15\": 0,\"50\": 4,\"51\": 4,\n",
    "             \"52\": 4, \"53\": 4,\"54\": 4, \"55\": 4, \"64\": 4,\"72\": 7, \"101\": 5,\"102\": 5,\"103\": 5, \"104\": 5,\n",
    "          \"105\": 5,\"106\": 5,\"107\": 5, \"108\": 5,\"109\": 5,\"110\": 5,\"111\": 5,\"120\": 5,\"121\": 5,\"150\": 6, \"151\": 6,\n",
    "      \"152\": 6,\"53\": 6, \"154\": 6,\"155\": 6, \"160\": 6, \"161\": 6, \"185\": 6, \"196\": 5,\n",
    "      \"197\": 5,\"200\": 6,\"201\": 6, \"202\": 6,\"211\": 6,\"212\": 6,\"213\": 6, \"221\": 6, \"222\": 6,\"223\": 6\n",
    "  }\n",
    "print(conv_dict1)\n",
    "\n",
    "model1 = {\"0\":1, \"1\":2, \"2\":5, \"3\":6, \"4\":59, \"5\":196, \"6\": 211, \"7\":72}\n",
    "\n",
    "#\n",
    "\n",
    "direct_conv1={k:j for k, v in conv_dict1.items() for  i, j in model1.items()  if int(i) == v }\n",
    "\n",
    "print(direct_conv1)\n",
    "direct1 = [direct_conv1.get(str(k)) for k in data]\n",
    "\n",
    "conv_dict2 = {\"1\": 0, \"2\": 1, \"3\": 0, \"4\": 2, \"5\": 2, \"6\": 3, \"7\": 6,  \"12\": 0, \"13\": 0, \"15\": 0, \"50\": 0,\n",
    "     \"51\": 0, \"52\": 0, \"53\": 0,\"54\": 0, \"55\": 0,  \"64\": 0,\"72\": 0,\"101\": 0,\"102\": 4,\"103\": 4, \"104\": 4,\n",
    "      \"105\": 4,\"106\": 0,\"107\": 0,\"108\": 0,\"109\": 0, \"110\": 0,\"111\": 0, \"120\": 0, \"121\": 0,\"150\": 0,\n",
    "      \"151\": 0,\"152\": 5,\"153\": 5,\"154\": 5, \"155\": 5,\"160\": 0,\"161\": 0,\"185\": 0,\"196\": 4,\"197\": 4,\n",
    "      \"200\": 5,\"201\": 5,\"202\": 5,\"211\": 5,\"212\": 5, \"213\": 5,\"221\": 5,\"222\": 5, \"223\": 5\n",
    "     }\n",
    "model2 = {\"0\":1, \"1\":2, \"2\":5, \"3\":6, \"4\":196, \"5\":211}\n",
    "direct_conv2={k:j for k, v in conv_dict2.items() for  i, j in model2.items()  if int(i) == v }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "direct2 = [direct_conv2.get(str(k)) for k in data]\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "# Supposons que direct soit votre tableau NumPy\n",
    "# Nettoyer le tableau pour supprimer les valeurs None\n",
    "cleaned_direct = [x for x in direct1 if x is not None]\n",
    "\n",
    "# Utiliser np.unique() sur le tableau nettoyé\n",
    "unique_values = np.unique(cleaned_direct)\n",
    "\n",
    "# Afficher les valeurs uniques\n",
    "print(\"unique values\")\n",
    "print(type(unique_values))\n",
    "print(unique_values)\n",
    "\n",
    "print(type(np.array(direct1)))\n",
    "# Convertir les valeurs de data en utilisant le dictionnaire de conversion\n",
    "\n",
    "# Convertir les valeurs de data en utilisant le dictionnaire de conversion\n",
    "converted_data1 = [conv_dict1.get(str(value), value) for value in data]\n",
    "print(np.unique(converted_data1))\n",
    "\n",
    "converted_data2 = [conv_dict2.get(str(value), value) for value in data]\n",
    "print(np.unique(converted_data2))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "conv_class = {\n",
    "        \"0\": \"default\",\n",
    "        \"1\": \"ground\",\n",
    "        \"2\": \"vegetation\",\n",
    "        \"3\": \"building\",\n",
    "        \"4\": \"structure\",\n",
    "        \"5\": \"cable\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 113550333/113550333 [00:12<00:00, 9400057.42it/s] \n",
      "100%|██████████| 113550333/113550333 [00:12<00:00, 9164199.40it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique classe predictions\n",
      "113550333\n",
      "[  1   2   5   6 196 211]\n",
      "\n",
      "unique classe ground truth\n",
      "\n",
      "len gtruth_class 113550333\n",
      "gtruth unique classes [  2   3   4   5   6  14  50  51  56  70  71  72  79 106 107 111 150 151\n",
      " 161 196 197 211 212 213]\n",
      "\n",
      "len gtruth_class_merged 113550333\n",
      "unique gtruth lasses merged classe [  1   2   5   6  14  56  70  71  79 196 211]\n",
      "len ground_truth 113550333\n",
      "len pred 113550333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 113550333/113550333 [06:51<00:00, 275946.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrice de confusion :/n               Classe 1   Classe 2    Classe 5  Classe 6  Classe 14  Classe 56  \\\n",
      "Classe 1    33115384.0  1846776.0    458013.0  450914.0      248.0    16148.0   \n",
      "Classe 2    12549409.0  1433781.0      3885.0   30323.0        0.0      333.0   \n",
      "Classe 5     8162659.0   267088.0  53900527.0  207145.0      695.0     2778.0   \n",
      "Classe 6      480644.0    40658.0     35627.0  374585.0      392.0        0.0   \n",
      "Classe 14          0.0        0.0         0.0       0.0        0.0        0.0   \n",
      "Classe 56          0.0        0.0         0.0       0.0        0.0        0.0   \n",
      "Classe 70          0.0        0.0         0.0       0.0        0.0        0.0   \n",
      "Classe 71          0.0        0.0         0.0       0.0        0.0        0.0   \n",
      "Classe 79          0.0        0.0         0.0       0.0        0.0        0.0   \n",
      "Classe 196      4890.0        1.0     46356.0      79.0      671.0       15.0   \n",
      "Classe 211     10397.0      377.0       968.0       0.0      743.0        0.0   \n",
      "\n",
      "            Classe 70  Classe 71  Classe 79  Classe 196  Classe 211  \n",
      "Classe 1        509.0     2299.0     8866.0      2387.0         0.0  \n",
      "Classe 2          0.0        0.0       85.0        11.0        96.0  \n",
      "Classe 5        710.0      630.0     2935.0      4262.0         0.0  \n",
      "Classe 6        258.0        7.0      183.0        28.0         0.0  \n",
      "Classe 14         0.0        0.0        0.0         0.0         0.0  \n",
      "Classe 56         0.0        0.0        0.0         0.0         0.0  \n",
      "Classe 70         0.0        0.0        0.0         0.0         0.0  \n",
      "Classe 71         0.0        0.0        0.0         0.0         0.0  \n",
      "Classe 79         0.0        0.0        0.0         0.0         0.0  \n",
      "Classe 196      102.0        0.0        0.0      7112.0       167.0  \n",
      "Classe 211        0.0        0.0        0.0        39.0     77138.0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ysouley\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\ysouley\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Valeurs</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Métriques</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>IoU</th>\n",
       "      <td>0.692586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.782988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.862058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.802300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1_score</th>\n",
       "      <td>0.782988</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Valeurs\n",
       "Métriques          \n",
       "IoU        0.692586\n",
       "Recall     0.782988\n",
       "precision  0.862058\n",
       "Accuracy   0.802300\n",
       "F1_score   0.782988"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_dict2 = {\"1\": 0, \"2\": 1, \"3\": 0, \"4\": 2, \"5\": 2, \"6\": 3, \"7\": 6,  \"12\": 0, \"13\": 0, \"15\": 0, \"50\": 0,\n",
    "     \"51\": 0, \"52\": 0, \"53\": 0,\"54\": 0, \"55\": 0,  \"64\": 0,\"72\": 0,\"101\": 0,\"102\": 4,\"103\": 4, \"104\": 4,\n",
    "      \"105\": 4,\"106\": 0,\"107\": 0,\"108\": 0,\"109\": 0, \"110\": 0,\"111\": 0, \"120\": 0, \"121\": 0,\"150\": 0,\n",
    "      \"151\": 0,\"152\": 5,\"153\": 5,\"154\": 5, \"155\": 5,\"160\": 0,\"161\": 0,\"185\": 0,\"196\": 4,\"197\": 4,\n",
    "      \"200\": 5,\"201\": 5,\"202\": 5,\"211\": 5,\"212\": 5, \"213\": 5,\"221\": 5,\"222\": 5, \"223\": 5\n",
    "     }\n",
    "model2 = {\"0\":1, \"1\":2, \"2\":5, \"3\":6, \"4\":196, \"5\":211}\n",
    "direct_conv2={k:j for k, v in conv_dict2.items() for  i, j in model2.items()  if int(i) == v }\n",
    "\n",
    "\n",
    "\n",
    "# Exemple d'utilisation\n",
    "rte2_pred=read_file_class(r\"E:/S_Données_pour apprentissage/data_output inference/Donnees  labelises en sortie d'inference/rte3/9a275f9320714938a25097c942b35477/preds/000006.laz\")\n",
    "rte2_truth=read_file_class(r\"E:/S_Données_pour apprentissage/Données pour prédiction sans bruit/000006.laz\")\n",
    "pred_class=num_points(rte2_pred)\n",
    "gtruth_class=num_points(rte2_truth)\n",
    "\n",
    "#num_ground = [x for x in num_ground if x is not None]\n",
    "#num_ground=[direct_conv2.get(str(k)) for k in gtruth_class]\n",
    "gtruth_cleanead = [direct_conv2.get(str(k)) if direct_conv2.get(str(k)) is not None else k for k in gtruth_class]\n",
    "\n",
    "rte2_predictions = np.array(pred_class)\n",
    "print(\"unique classe predictions\")\n",
    "print(len(rte2_predictions))\n",
    "print(np.unique(rte2_predictions))\n",
    "print(\"\")\n",
    "\n",
    "rte2_ground_truth =np.array(gtruth_cleanead)\n",
    "print(\"unique classe ground truth\")\n",
    "print(\"\")\n",
    "print(\"len gtruth_class\",len(gtruth_class))\n",
    "print(\"gtruth unique classes\",np.unique(gtruth_class))\n",
    "print(\"\")\n",
    "print(\"len gtruth_class_merged\", len(rte2_ground_truth))\n",
    "print(\"unique gtruth lasses merged classe\", np.unique(rte2_ground_truth))\n",
    "\n",
    "\n",
    "#class_titles = [\"Classe 0\", \"Classe 1\",\"classes2\",\"classe3\"]\n",
    "conf_matrix = compute_confusion_matrix(rte2_ground_truth,rte2_predictions)\n",
    "\n",
    "print(\"Matrice de confusion :/n\", conf_matrix)\n",
    "\n",
    "## Instancing\n",
    "rte2_metrics=Metrics(rte2_ground_truth,rte2_predictions)\n",
    "## calling a methods for computing metrics\n",
    "rte2=rte2_metrics.compute_metrics()\n",
    "rte2_metrics.weighted_metrics()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Valeurs des métriques</th>\n",
       "      <th>Classe 1</th>\n",
       "      <th>Classe 2</th>\n",
       "      <th>Classe 5</th>\n",
       "      <th>Classe 6</th>\n",
       "      <th>Classe 14</th>\n",
       "      <th>Classe 56</th>\n",
       "      <th>Classe 70</th>\n",
       "      <th>Classe 71</th>\n",
       "      <th>Classe 79</th>\n",
       "      <th>Classe 196</th>\n",
       "      <th>Classe 211</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Métriques</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>IoU</th>\n",
       "      <td>0.579857</td>\n",
       "      <td>0.088654</td>\n",
       "      <td>0.854286</td>\n",
       "      <td>0.231105</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.107562</td>\n",
       "      <td>0.857804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.609597</td>\n",
       "      <td>0.399529</td>\n",
       "      <td>0.989993</td>\n",
       "      <td>0.352370</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.513910</td>\n",
       "      <td>0.996602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.922394</td>\n",
       "      <td>0.102282</td>\n",
       "      <td>0.861727</td>\n",
       "      <td>0.401751</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.119745</td>\n",
       "      <td>0.860320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1_score</th>\n",
       "      <td>0.734063</td>\n",
       "      <td>0.162869</td>\n",
       "      <td>0.921417</td>\n",
       "      <td>0.375443</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.194232</td>\n",
       "      <td>0.923460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.782988</td>\n",
       "      <td>0.782988</td>\n",
       "      <td>0.782988</td>\n",
       "      <td>0.782988</td>\n",
       "      <td>0.782988</td>\n",
       "      <td>0.782988</td>\n",
       "      <td>0.782988</td>\n",
       "      <td>0.782988</td>\n",
       "      <td>0.782988</td>\n",
       "      <td>0.782988</td>\n",
       "      <td>0.782988</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Valeurs des métriques  Classe 1  Classe 2  Classe 5  Classe 6  Classe 14  \\\n",
       "Métriques                                                                  \n",
       "IoU                    0.579857  0.088654  0.854286  0.231105   0.000000   \n",
       "Recall                 0.609597  0.399529  0.989993  0.352370   0.000000   \n",
       "Precision              0.922394  0.102282  0.861727  0.401751   0.000000   \n",
       "F1_score               0.734063  0.162869  0.921417  0.375443   0.000000   \n",
       "Accuracy               0.782988  0.782988  0.782988  0.782988   0.782988   \n",
       "\n",
       "Valeurs des métriques  Classe 56  Classe 70  Classe 71  Classe 79  Classe 196  \\\n",
       "Métriques                                                                       \n",
       "IoU                     0.000000   0.000000   0.000000   0.000000    0.107562   \n",
       "Recall                  0.000000   0.000000   0.000000   0.000000    0.513910   \n",
       "Precision               0.000000   0.000000   0.000000   0.000000    0.119745   \n",
       "F1_score                0.000000   0.000000   0.000000   0.000000    0.194232   \n",
       "Accuracy                0.782988   0.782988   0.782988   0.782988    0.782988   \n",
       "\n",
       "Valeurs des métriques  Classe 211  \n",
       "Métriques                          \n",
       "IoU                      0.857804  \n",
       "Recall                   0.996602  \n",
       "Precision                0.860320  \n",
       "F1_score                 0.923460  \n",
       "Accuracy                 0.782988  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rte2_metrics.metrics_frame(rte2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rte2_metrics.weighted_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infos_class(r\"D:/YSO/Nouveau dossier/Controle lignes elec-1_YAM.laz\")\n",
    "print(\"\")\n",
    "infos_class(r\"D:/YSO/Controle lignes elec-1_YAM.laz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of Correcting the input issue of converrted data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rte2: second predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model2 : file :000006.laz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple d'utilisation\n",
    "rte2_pred=read_file_class(r\"E:/S_Données_pour apprentissage/data_output inference/Donnees  labelises en sortie d'inference/rte2 correctement labelise/99385bdd1b4245d6813754f478bd5f2c/preds/000006.laz\")\n",
    "rte2_truth=read_file_class(r\"E:/S_Données_pour apprentissage/Données pour prédiction sans bruit/000006.laz\")\n",
    "pred_class=num_points(rte2_pred)\n",
    "gtruth_class=num_points(rte2_truth)\n",
    "\n",
    "num_ground = [direct_conv2.get(str(k)) if direct_conv2.get(str(k)) is not None else k for k in gtruth_class]\n",
    "#num_ground=[direct_conv2.get(str(k)) for k in gtruth_class]\n",
    "gtruth_cleanead = [x for x in num_ground if x is not None]\n",
    "\n",
    "rte2_predictions = np.array(pred_class)\n",
    "print(\"unique classe predictions\")\n",
    "print(len(rte2_predictions))\n",
    "print(np.unique(rte2_predictions))\n",
    "print(\"\")\n",
    "\n",
    "rte2_ground_truth =np.array(gtruth_cleanead)\n",
    "print(\"unique classe ground truth\")\n",
    "print(\"\")\n",
    "print(\"len gtruth_class\",len(gtruth_class))\n",
    "print(\"gtruth unique classes\",np.unique(gtruth_class))\n",
    "print(\"\")\n",
    "print(\"len gtruth_class_merged\", len(rte2_ground_truth))\n",
    "print(\"unique gtruth lasses merged classe\", np.unique(rte2_ground_truth))\n",
    "\n",
    "\n",
    "#class_titles = [\"Classe 0\", \"Classe 1\",\"classes2\",\"classe3\"]\n",
    "conf_matrix = compute_confusion_matrix(rte2_ground_truth,rte2_predictions)\n",
    "\n",
    "print(\"Matrice de confusion :/n\", conf_matrix)\n",
    "\n",
    "## Instancing\n",
    "rte2_metrics=Metrics(rte2_ground_truth,rte2_predictions)\n",
    "## calling a methods for computing metrics\n",
    "rte2=rte2_metrics.compute_metrics()\n",
    "rte2_metrics.weighted_metrics()\n",
    "## for displaying as frame\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## jasonload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rte2_metrics.metrics_frame(rte2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rte2_metrics.graphmetrics(rte2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rte2_metrics.n_graphmetrics(rte2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model1: file 000006.laz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple d'utilisation\n",
    "rte1_pred=read_file_class(r\"E:/S_Données_pour apprentissage/data_output inference/Donnees  labelises en sortie d'inference/rte1 correctement labelise/db3ea63f541a4d9fbe68bd96f98a86ae/preds/000006.laz\")\n",
    "rte1_truth=read_file_class(r\"E:/S_Données_pour apprentissage/Données pour prédiction sans bruit/000006.laz\")\n",
    "pred_class=num_points(rte1_pred)\n",
    "gtruth_class=num_points(rte1_truth)\n",
    "\n",
    "num_ground = [direct_conv1.get(str(k)) if direct_conv1.get(str(k)) is not None else k for k in gtruth_class]\n",
    "#num_ground=[direct_conv1.get(str(k)) for k in gtruth_class]\n",
    "gtruth_cleanead = [x for x in num_ground if x is not None]\n",
    "\n",
    "rte1_predictions = np.array(pred_class)\n",
    "print(\"unique classe predictions\")\n",
    "print(len(rte1_predictions))\n",
    "print(np.unique(rte1_predictions))\n",
    "print(\"\")\n",
    "\n",
    "rte1_ground_truth =np.array(gtruth_cleanead)\n",
    "print(\"unique classe ground truth\")\n",
    "print(\"\")\n",
    "print(\"len gtruth_class\",len(gtruth_class))\n",
    "print(\"gtruth unique classes\",np.unique(gtruth_class))\n",
    "print(\"\")\n",
    "print(\"len gtruth_class_merged\", len(rte1_ground_truth))\n",
    "print(\"unique gtruth lasses merged classe\", np.unique(rte1_ground_truth))\n",
    "\n",
    "\n",
    "#class_titles = [\"Classe 0\", \"Classe 1\",\"classes2\",\"classe3\"]\n",
    "conf_matrix = compute_confusion_matrix(rte1_ground_truth,rte1_predictions)\n",
    "\n",
    "print(\"Matrice de confusion :/n\", conf_matrix)\n",
    "\n",
    "## Instancing\n",
    "rte1_metrics=Metrics(rte1_ground_truth,rte1_predictions)\n",
    "## calling a methods for computing metrics\n",
    "rte1=rte1_metrics.compute_metrics()\n",
    "\n",
    "## for displaying as frame\n",
    "rte1_metrics.graphmetrics(rte1)\n",
    "rte1_metrics.n_graphmetrics(rte1)\n",
    "\n",
    "\n",
    "rte1_metrics.weighted_metrics()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rte1_metrics.metrics_frame(rte1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model2 : file Controle lignes elec-1_YAM.laz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple d'utilisation\n",
    "rte2_pred=read_file_class(r\"D:/YSO/Nouveau dossier/Controle lignes elec-1_YAM.laz\")\n",
    "rte2_truth=read_file_class(r\"D:/YSO/Controle lignes elec-1_YAM.laz\")\n",
    "pred_class=num_points(rte2_pred)\n",
    "gtruth_class=num_points(rte2_truth)\n",
    "\n",
    "num_ground = [direct_conv2.get(str(k)) if direct_conv2.get(str(k)) is not None else k for k in gtruth_class]\n",
    "#num_ground=[direct_conv2.get(str(k)) for k in gtruth_class]\n",
    "gtruth_cleanead = [x for x in num_ground if x is not None]\n",
    "\n",
    "rte2_predictions = np.array(pred_class)\n",
    "print(\"unique classe predictions\")\n",
    "print(len(rte2_predictions))\n",
    "print(np.unique(rte2_predictions))\n",
    "print(\"\")\n",
    "rte2_ground_truth =np.array(gtruth_cleanead)\n",
    "print(\"unique classe ground truth\")\n",
    "print(\"\")\n",
    "print(\"gtruth_class\",len(gtruth_class))\n",
    "print(len(rte2_ground_truth))\n",
    "print(np.unique(rte2_ground_truth))\n",
    "\n",
    "\"\"\"print(\"type and unique pred_calss\")\n",
    "print(type(pred_class))\n",
    "print(np.unique(pred_class))\n",
    "print(\"rte2_pred\")\n",
    "print(np.unique(rte2_predictions))\n",
    "print(type(rte2_predictions))\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "print(\"cleanead\")\n",
    "print(type(gtruth_cleanead))\n",
    "print(\"rte2_gtruth\")\n",
    "print(type(rte2_ground_truth))\n",
    "\n",
    "\n",
    "\n",
    "print(\"gtruth_class\")\n",
    "print(np.unique(gtruth_class))\n",
    "print(type(gtruth_class))\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#class_titles = [\"Classe 0\", \"Classe 1\",\"classes2\",\"classe3\"]\n",
    "conf_matrix = compute_confusion_matrix(rte2_ground_truth,rte2_predictions)\n",
    "\n",
    "print(\"Matrice de confusion :\\n\", conf_matrix)\n",
    "\n",
    "## Instancing\n",
    "rte2_metrics=Metrics(rte2_ground_truth,rte2_predictions)\n",
    "## calling a methods for computing metrics\n",
    "rte2=rte2_metrics.compute_metrics()\n",
    "rte2_metrics.weighted_metrics()\n",
    "## for displaying as frame\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "rte2_metrics.graphmetrics(rte2)\n",
    "rte2_metrics.n_graphmetrics(rte2)\n",
    "\n",
    "## jasonload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rte2_metrics.metrics_frame(rte2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rte2_metrics.weighted_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count_points_per_class(r\"D:/YSO/Controle lignes elec-1_YAM.laz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rte1 first predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model1 file Controle lignes elec-1_YAM.laz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Exemple d'utilisation\n",
    "rte1_pred=read_file_class(r\"D:/YSO/rte1/Controle lignes elec-1_YAM.laz\")\n",
    "rte1_truth=read_file_class(r\"D:/YSO/Controle lignes elec-1_YAM.laz\")\n",
    "pred_class=num_points(rte1_pred)\n",
    "gtruth_class=num_points(rte1_truth)\n",
    "\n",
    "num_ground = [direct_conv2.get(str(k)) if direct_conv2.get(str(k)) is not None else k for k in gtruth_class]\n",
    "#num_ground=[direct_conv2.get(str(k)) for k in gtruth_class]\n",
    "gtruth_cleanead = [x for x in num_ground if x is not None]\n",
    "\n",
    "rte1_predictions = np.array(pred_class)\n",
    "print(\"unique classe predictions\")\n",
    "print(len(rte1_predictions))\n",
    "print(np.unique(rte1_predictions))\n",
    "print(\"\")\n",
    "rte1_ground_truth =np.array(gtruth_cleanead)\n",
    "print(\"unique classe ground truth\")\n",
    "print(\"\")\n",
    "print(\"gtruth_class\",len(gtruth_class))\n",
    "print(len(rte1_ground_truth))\n",
    "print(np.unique(rte1_ground_truth))\n",
    "\n",
    "\"\"\"print(\"type and unique pred_calss\")\n",
    "print(type(pred_class))\n",
    "print(np.unique(pred_class))\n",
    "print(\"rte1_pred\")\n",
    "print(np.unique(rte1_predictions))\n",
    "print(type(rte1_predictions))\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "print(\"cleanead\")\n",
    "print(type(gtruth_cleanead))\n",
    "print(\"rte1_gtruth\")\n",
    "print(type(rte1_ground_truth))\n",
    "\n",
    "\n",
    "\n",
    "print(\"gtruth_class\")\n",
    "print(np.unique(gtruth_class))\n",
    "print(type(gtruth_class))\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#class_titles = [\"Classe 0\", \"Classe 1\",\"classes2\",\"classe3\"]\n",
    "conf_matrix = compute_confusion_matrix(rte1_ground_truth,rte1_predictions)\n",
    "\n",
    "print(\"Matrice de confusion :\\n\", conf_matrix)\n",
    "\n",
    "## Instancing\n",
    "rte1_metrics=Metrics(rte1_ground_truth,rte1_predictions)\n",
    "## calling a methods for computing metrics\n",
    "rte1=rte1_metrics.compute_metrics()\n",
    "rte1_metrics.weighted_metrics()\n",
    "## for displaying as frame\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "rte1_metrics.graphmetrics(rte1)\n",
    "rte1_metrics.n_graphmetrics(rte1)\n",
    "\n",
    "## jasonload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rte1_metrics.metrics_frame(rte1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rte1_metrics.weighted_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part without mapping the input file with json config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple d'utilisation\n",
    "rte2_pred=read_file_class(r\"E:\\S_Données_pour apprentissage\\data_post_Pred\\Nouveau dossier\\99385bdd1b4245d6813754f478bd5f2c\\preds\\000006.laz\")\n",
    "rte2_truth=read_file_class(r\"E:\\S_Données_pour apprentissage\\Données pour prédiction sans bruit\\000006.laz\")\n",
    "num_classe1=num_points(rte2_pred)\n",
    "num_classe2=num_points(rte2_truth)\n",
    "rte2_predictions = np.array(num_classe1)\n",
    "rte2_ground_truth = np.array(num_classe2)\n",
    "#class_titles = [\"Classe 0\", \"Classe 1\",\"classes2\",\"classe3\"]\n",
    "conf_matrix = compute_confusion_matrix(rte2_ground_truth,rte2_predictions)\n",
    "\n",
    "print(\"Matrice de confusion :\\n\", conf_matrix)\n",
    "\n",
    "## Instancing\n",
    "rte2_metrics=Metrics(rte2_ground_truth,rte2_predictions)\n",
    "## calling a methods for computing metrics\n",
    "rte2=rte2_metrics.compute_metrics()\n",
    "rte2_metrics.weighted_metrics()\n",
    "## for displaying as frame\n",
    "rte2_metrics.metrics_frame(rte2)\n",
    "\n",
    "#metrics.graphmetrics(rte2)\n",
    "\n",
    "\n",
    "#compute_metrics_per_class(rte2_ground_truth,rte2_predictions)\n",
    "#calculate_metrics(conf_matrix.values)\n",
    "\n",
    "\n",
    "#calculate_metrics(conf_matrix.values)\n",
    "\n",
    "# Écrire le Datrte2rame dans un fichier CSV\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rte2_metrics.weighted_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rte2_metrics.graphmetrics(rte2)\n",
    "rte2_metrics.n_graphmetrics(rte2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple d'utilisation\n",
    "c1=read_file_class(r\"E:\\RTE_entrainement\\IA_Train_Valid_Test\\Dev_YSO_py\\pred_25.laz\")\n",
    "c2=read_file_class(r\"E:\\RTE_entrainement\\IA_Train_Valid_Test\\Dev_YSO_py\\before_25.laz\")\n",
    "num_classe1=num_points(c1)\n",
    "num_classe2=num_points(c2)\n",
    "predictions = np.array(num_classe1)\n",
    "ground_truth = np.array(num_classe2)\n",
    "#class_titles = [\"Classe 0\", \"Classe 1\",\"classes2\",\"classe3\"]\n",
    "conf_matrix = compute_confusion_matrix(ground_truth,predictions)\n",
    "\n",
    "print(\"Matrice de confusion :\\n\", conf_matrix)\n",
    "\n",
    "## Instancing\n",
    "metrics=Metrics(ground_truth,predictions)\n",
    "## calling a methods for computing metrics\n",
    "af=metrics.compute_metrics()\n",
    "metrics.weighted_metrics()\n",
    "## for displaying as frame\n",
    "metrics.metrics_frame(af)\n",
    "\n",
    "#metrics.graphmetrics(af)\n",
    "\n",
    "\n",
    "#compute_metrics_per_class(ground_truth,predictions)\n",
    "#calculate_metrics(conf_matrix.values)\n",
    "\n",
    "\n",
    "#calculate_metrics(conf_matrix.values)\n",
    "\n",
    "# Écrire le DataFrame dans un fichier CSV\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.weighted_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Exemple d'utilisation\n",
    "c1=read_file_class(r\"E:\\RTE_entrainement\\IA_Train_Valid_Test\\Dev_YSO_py\\pred_25.laz\")\n",
    "c2=read_file_class(r\"E:\\RTE_entrainement\\IA_Train_Valid_Test\\Dev_YSO_py\\before_25.laz\")\n",
    "num_classe1=num_points(c1)\n",
    "num_classe2=num_points(c2)\n",
    "predictions = np.array(num_classe1)\n",
    "ground_truth = np.array(num_classe2)\n",
    "#class_titles = [\"Classe 0\", \"Classe 1\",\"classes2\",\"classe3\"]\n",
    "#conf_matrix = compute_confusion_matrix(ground_truth,predictions)\n",
    "\n",
    "print(\"Matrice de confusion :\\n\", conf_matrix)\n",
    "\n",
    "## Instancing\n",
    "metrics=Metrics(ground_truth,predictions)\n",
    "## calling a methods for computing metrics\n",
    "af=metrics.compute_metrics()\n",
    "metrics.graphmetrics(af)\n",
    "metrics.n_graphmetrics(af)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
